{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preferential Bayesian Optimization: EI\n",
    "This notebook demonstrates the use of the Expected Improvement (EI) acquisition function on ordinal (preference) data.\n",
    "\n",
    "Formulation by Nguyen Quoc Phong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from gpflow.utilities import set_trainable, print_summary\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.split(os.path.split(os.path.split(os.getcwd())[0])[0])[0]) # Move 3 levels up directory to import PBO\n",
    "import PBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = PBO.objectives.forrester\n",
    "objective_low = 0.\n",
    "objective_high = 1.\n",
    "objective_name = \"Forrester\"\n",
    "acquisition_name = \"EI\"\n",
    "experiment_name = \"PBO\" + \"_\" + acquisition_name + \"_\" + objective_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 20\n",
    "num_evals = 20\n",
    "num_samples = 100\n",
    "num_choices = 2\n",
    "input_dims = 1\n",
    "num_maximizers = 20\n",
    "num_init_points = 3\n",
    "num_inducing_init = 3\n",
    "num_discrete_per_dim = 10000 # Discretization of continuous input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.getcwd() + '/results/' + experiment_name + '/'\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.makedirs(results_dir)\n",
    "    print(\"Directory \" , results_dir ,  \" created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , results_dir ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Forrester function (global min at ~0.757):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(0.0, 1.0, 100).reshape(100, 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(xx, objective(xx), 'C0', linewidth=1)\n",
    "plt.xlim(-0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(model, X, y, title, cmap=\"Spectral\"):\n",
    "    #Plotting code from GPflow authors\n",
    "\n",
    "    ## generate test points for prediction\n",
    "    xx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)  # test points must be of shape (N, D)\n",
    "\n",
    "    ## predict mean and variance of latent GP at test points\n",
    "    mean, var = model.predict_f(xx)\n",
    "\n",
    "    ## generate 10 samples from posterior\n",
    "    samples = model.predict_f_samples(xx, 10)  # shape (10, 100, 1)\n",
    "\n",
    "    ## plot \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, y, 'kx', mew=2)\n",
    "    plt.plot(xx, mean, 'C0', lw=2)\n",
    "    plt.fill_between(xx[:,0],\n",
    "                     mean[:,0] - 1.96 * np.sqrt(var[:,0]),\n",
    "                     mean[:,0] + 1.96 * np.sqrt(var[:,0]),\n",
    "                     color='C0', alpha=0.2)\n",
    "\n",
    "    plt.plot(xx, samples[:, :, 0].numpy().T, 'C0', linewidth=.5)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.savefig(fname=results_dir + title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_observation(X, objective):\n",
    "    f = PBO.objectives.objective_get_f_neg(X, objective)\n",
    "    return PBO.observation_model.gen_observation_from_f(X, f, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_visualize(X, y, num_inducing, title):\n",
    "    \n",
    "    # Train model with data\n",
    "    q_mu, q_sqrt, u, inputs, k, indifference_threshold = PBO.models.learning_stochastic.train_model_fullcov(X, y, \n",
    "                                                                         num_inducing=num_inducing,\n",
    "                                                                         obj_low=objective_low,\n",
    "                                                                         obj_high=objective_high,\n",
    "                                                                         lengthscale=0.05,\n",
    "                                                                         num_steps=3000)\n",
    "    likelihood = gpflow.likelihoods.Gaussian()\n",
    "    model = PBO.models.learning.init_SVGP_fullcov(q_mu, q_sqrt, u, k, likelihood)\n",
    "    u_mean = q_mu.numpy()\n",
    "    inducing_vars = u.numpy()\n",
    "    \n",
    "    # Visualize model\n",
    "    plot_gp(model, inducing_vars, u_mean, title)\n",
    "    \n",
    "    return model, inputs, u_mean, inducing_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_grid(input_dims, num_discrete_per_dim, low=0., high=1.):\n",
    "    \"\"\"\n",
    "    Returns an array with all possible permutations of discrete values in input_dims number of dimensions.\n",
    "    :param input_dims: int\n",
    "    :param num_discrete_per_dim: int\n",
    "    :param low: int\n",
    "    :param high: int\n",
    "    :return: tensor of shape (num_discrete_per_dim ** input_dims, input_dims)\n",
    "    \"\"\"\n",
    "    num_points = num_discrete_per_dim ** input_dims\n",
    "    out = np.zeros([num_points, input_dims])\n",
    "    discrete_points = np.linspace(low, high, num_discrete_per_dim)\n",
    "    for i in range(num_points):\n",
    "        for dim in range(input_dims):\n",
    "            val = num_discrete_per_dim ** (dim)\n",
    "            out[i, dim] = discrete_points[int((i // val) % num_discrete_per_dim)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is our main metric for the performance of the acquisition function: The closer the model's best guess to the target (in this case, the global minimum of the Forrester function), the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_guess(model):\n",
    "    \"\"\"\n",
    "    :return: tensor of shape (input_dims) a GP model's best guess of the global maximum of f.\n",
    "    \"\"\"\n",
    "    xx = uniform_grid(input_dims, num_discrete_per_dim, low=objective_low, high=objective_high)\n",
    "    res = model.predict_f(xx)[0].numpy()\n",
    "    return xx[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in these arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_at_end = int((num_init_points-1) * num_init_points / 2 + num_evals)\n",
    "X_results = np.zeros([num_runs, num_data_at_end, num_choices, input_dims])\n",
    "y_results = np.zeros([num_runs, num_data_at_end, 1, input_dims])\n",
    "best_guess_results = np.zeros([num_runs, num_evals, input_dims])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the initial values for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "init_points = np.random.uniform(low=objective_low, high=objective_high, size=[num_runs, num_init_points, input_dims])\n",
    "num_combs = int((num_init_points-1) * num_init_points / 2)\n",
    "init_vals = np.zeros([num_runs, num_combs, num_choices, input_dims])\n",
    "for run in range(num_runs):\n",
    "    cur_idx = 0\n",
    "    for init_point in range(num_init_points-1):\n",
    "        for next_point in range(init_point+1, num_init_points):\n",
    "            init_vals[run, cur_idx, 0] = init_points[run, init_point]\n",
    "            init_vals[run, cur_idx, 1] = init_points[run, next_point]\n",
    "            cur_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loops carry out the Bayesian optimization algorithm over a number of runs, with a fixed number of evaluations per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(num_runs):\n",
    "    print(\"Beginning run %s\" % (run))\n",
    "    \n",
    "    X = init_vals[run]\n",
    "    y = get_noisy_observation(X, objective)\n",
    "    \n",
    "    model, inputs, u_mean, inducing_vars = train_and_visualize(X, y, num_inducing_init, \"Run_{}:_Initial_model\".format(run))\n",
    "\n",
    "    for evaluation in range(num_evals):\n",
    "        print(\"Beginning evaluation %s\" % (evaluation)) \n",
    "\n",
    "        # Get incumbent maximizer\n",
    "        maximizer = np.expand_dims(best_guess(model), axis=0)  # (1, input_dims)\n",
    "        \n",
    "        print(\"Maximizer:\")\n",
    "        print(maximizer)\n",
    "        \n",
    "        # Sample possible next input points. In EI, all queries are a pair with the incumbent maximizer as the \n",
    "        # first point and a next input point as the second point\n",
    "        \n",
    "        samples = np.random.uniform(low=objective_low,\n",
    "                                    high=objective_high,\n",
    "                                    size=(num_samples, input_dims))\n",
    "        \n",
    "        # Calculate EI vals\n",
    "        ei_vals = PBO.acquisitions.ei.EI(model, maximizer, samples)\n",
    "        \n",
    "        # Select query that maximizes EI\n",
    "        next_idx = np.argmax(ei_vals)\n",
    "        next_query = np.zeros((num_choices, input_dims))\n",
    "        next_query[0, :] = maximizer  # EI only works in binary choices\n",
    "        next_query[1, :] = samples[next_idx]\n",
    "        print(\"Evaluation %s: Next query is %s with EI value of %s\" % (evaluation, next_query, ei_vals[next_idx]))\n",
    "\n",
    "        X = np.concatenate([X, [next_query]])\n",
    "        # Evaluate objective function\n",
    "        y = np.concatenate([y, get_noisy_observation(np.expand_dims(next_query, axis=0), objective)], axis=0)\n",
    "        \n",
    "        print(\"Evaluation %s: Training model\" % (evaluation))\n",
    "        model, inputs, u_mean, inducing_vars = train_and_visualize(X, y, \n",
    "                                                                   num_inducing_init + evaluation + 1, \n",
    "                                                                   \"Run_{}_Evaluation_{}\".format(run, evaluation))\n",
    "\n",
    "        best_guess_results[run, evaluation, :] = best_guess(model)\n",
    "\n",
    "    X_results[run] = X\n",
    "    y_results[run] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1[3][0] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1[3] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals2 = train_model_fullcov(X1, y1, \n",
    "                                                                         num_inducing=4,\n",
    "                                                                         obj_low=objective_low,\n",
    "                                                                         obj_high=objective_high,\n",
    "                                                                         lengthscale=0.05,\n",
    "                                                                         num_steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(vals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#     1. use samples to compute the objective function\n",
    "#         elbo_fullcov(q_mu, q_sqrt_latent, inducing_variables, D_idxs, max_idxs, kernel, inputs)\n",
    "#        then it can works for different number of choices\n",
    "#     2. val_to_idxs, populate_dicts: need to adaptive to different number of choices\n",
    "#         by converting some to list\n",
    "#     2. check function q_f to see if the implementation is correct\n",
    "#         checked: correct\n",
    "\"\"\"\n",
    "    forester_get_Y:\n",
    "        as X is a list, change the function!\n",
    "    sample maximizers:\n",
    "        change observations from inducing input, inducing variables\n",
    "        to distribution of inducing variables\n",
    "\n",
    "Given ordinal (preference) data consisting of sets of input points and a most preferred input point for every such set,\n",
    "the train_model function learns variational parameters that approximate the distribution of a latent function f over\n",
    "all input points present in the data, which can be used to construct GP models to approximate f over the entire input\n",
    "space.\n",
    "Formulation by Nguyen Quoc Phong.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import gpflow\n",
    "from gpflow.utilities import set_trainable\n",
    "\n",
    "\n",
    "def elbo_fullcov(q_mu,\n",
    "                q_sqrt_latent,\n",
    "                inducing_inputs,\n",
    "                D_idxs,\n",
    "                max_idxs,\n",
    "                kernel,\n",
    "                inputs,\n",
    "                indifference_threshold,\n",
    "                n_inducing_sample=50,\n",
    "                n_f_given_inducing_sample=30):\n",
    "    \"\"\"\n",
    "    Calculates the ELBO for the PBO formulation, using a full covariance matrix.\n",
    "    :param q_mu: tensor with shape (num_inducing, 1)\n",
    "    :param q_sqrt_latent: tensor with shape (1, num_inducing, num_inducing). Will be forced into lower triangular\n",
    "        matrix such that q_sqrt @ q_sqrt^T represents the covariance matrix of inducing variables\n",
    "    :param inducing_inputs: tensor with shape (num_inducing, input_dims)\n",
    "    :param D_idxs: tensor with shape (num_data, num_choices, 1)\n",
    "        Input data points, that are indices into q_mu and q_var for tf.gather_nd\n",
    "    :param max_idxs: tensor with shape (num_data, 1)\n",
    "        Selection of most preferred input point for each collection of data points, that are indices into\n",
    "        q_mu and q_var\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: tensor of shape ()\n",
    "    \"\"\"\n",
    "    Kmm = kernel.K(inducing_inputs)\n",
    "\n",
    "    logdet_Kmm = tf.linalg.logdet(Kmm)\n",
    "    invKmm = cholesky_matrix_inverse(Kmm)\n",
    "\n",
    "    num_data = D_idxs.size()\n",
    "\n",
    "    # 1. Sample from q(u)\n",
    "    standard_mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=tf.zeros(tf.shape(q_mu)[0], dtype=tf.float64),\n",
    "            scale_diag=tf.ones(tf.shape(q_mu)[0], dtype=tf.float64))\n",
    "\n",
    "    standard_mvn_samples = standard_mvn.sample(n_inducing_sample)\n",
    "    # (n_inducing_sample, num_inducing)\n",
    "\n",
    "    q_sqrt = tf.linalg.band_part(q_sqrt_latent, -1, 0)  # Force into lower triangular\n",
    "    q_full = q_sqrt @ tf.linalg.matrix_transpose(q_sqrt)  # (1, num_data, num_data)\n",
    "    inv_q_full = cholesky_matrix_inverse(tf.squeeze(q_full, axis=0))\n",
    "\n",
    "    posterior_inducing_samples = q_sqrt @ tf.expand_dims(standard_mvn_samples, axis=-1) + q_mu\n",
    "    # (n_inducing_sample, num_inducing, 1)\n",
    "\n",
    "    # 2. p(f|u) where u are samples from q(u)\n",
    "    f_mean_given_inducing_sample, f_cov_given_inducing_sample = p_f_given_u(\n",
    "            posterior_inducing_samples,\n",
    "            inducing_inputs, kernel, inputs, invKmm)\n",
    "    # f_mean: (n_inducing_sample, num_inputs)\n",
    "    # f_cov: (num_inputs, num_inputs)\n",
    "\n",
    "    # 3. KL[q(u) || p(u)] = E_{q(u)} [log q(u)] - [log p(u)]\n",
    "    zero_mean_inducing_samples = posterior_inducing_samples - q_mu\n",
    "    # (n_inducing_sample, num_inducing, 1)\n",
    "\n",
    "    klterm = -0.5 * tf.reduce_mean(\n",
    "        tf.linalg.logdet(q_full)\n",
    "        + tf.transpose(zero_mean_inducing_samples, perm=[0,2,1]) @ inv_q_full @ zero_mean_inducing_samples\n",
    "        - logdet_Kmm\n",
    "        - tf.transpose(posterior_inducing_samples, perm= [0,2,1]) @ invKmm @ posterior_inducing_samples\n",
    "    )\n",
    "\n",
    "    def body(i, likelihood):\n",
    "        idxs = tf.squeeze(D_idxs.read(i))\n",
    "        max_idx = max_idxs[i]\n",
    "        num_choice = tf.shape(idxs)[0]\n",
    "\n",
    "        fi_cov = tf.gather(\n",
    "            tf.gather(f_cov_given_inducing_sample, indices=idxs, axis=0),\n",
    "            indices=idxs, axis=1)\n",
    "        # (num_choice, num_choice)\n",
    "\n",
    "        fi_mean = tf.gather(f_mean_given_inducing_sample, indices=idxs, axis=1)\n",
    "        # (n_inducing_sample, num_choice)\n",
    "\n",
    "        standard_mvn_i = tfp.distributions.MultivariateNormalDiag(\n",
    "                loc=tf.zeros_like(idxs, dtype=tf.float64),\n",
    "                scale_diag=tf.ones_like(idxs, dtype=tf.float64))\n",
    "\n",
    "        standard_mvn_i_samples = standard_mvn_i.sample(n_f_given_inducing_sample)\n",
    "        # (n_f_given_inducing_sample, num_choice)\n",
    "\n",
    "        eigvalues, eigvects = tf.linalg.eigh(fi_cov)\n",
    "        eigvalues = tf.clip_by_value(eigvalues, clip_value_min=0., clip_value_max=np.infty)\n",
    "        transform_mat = eigvects @ tf.linalg.diag(tf.sqrt(eigvalues))\n",
    "\n",
    "        zero_mean_f_samples = tf.squeeze(transform_mat @ tf.expand_dims(standard_mvn_i_samples, axis=-1), axis=-1)\n",
    "        # (n_f_given_inducing_sample, num_choice)\n",
    "\n",
    "        f_samples = tf.expand_dims(zero_mean_f_samples, axis=1) + fi_mean\n",
    "        # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "        # implementing a threshold for the choice of indifference\n",
    "        mask_mat = tf.eye(num_choice, dtype=tf.float64)\n",
    "        diff_mat = (1.0 - mask_mat) * indifference_threshold\n",
    "\n",
    "        def true_fn(max_idx, f_samples, diff_mat):\n",
    "            f_samples = f_samples + tf.gather(diff_mat, indices=max_idx, axis=0)\n",
    "\n",
    "            return tf.reduce_mean(\n",
    "                    tf.gather(f_samples, indices=max_idx, axis=-1)\n",
    "                    - tf.reduce_logsumexp(f_samples, axis=-1))\n",
    "\n",
    "        def false_fn(f_samples, mask_mat, diff_mat):\n",
    "            max_idx_f_samples = tf.squeeze(mask_mat @ tf.expand_dims(f_samples, axis=-1), axis=-1)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "            all_f_samples = tf.expand_dims(f_samples, axis=-1) + tf.gather(diff_mat, indices=max_idx, axis=0)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice, num_choice)\n",
    "\n",
    "            all_choice_logprob = max_idx_f_samples - tf.reduce_logsumexp(all_f_samples, axis=-2)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "            indifference_prob = 1.0 - tf.exp( tf.reduce_logsumexp(all_choice_logprob, axis=-1) )\n",
    "            indifference_prob = tf.clip_by_value(indifference_prob, clip_value_min=1e-50, clip_value_max=1.0 - 1e-50)\n",
    "            indifference_logprob = tf.math.log(indifference_prob)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample)\n",
    "\n",
    "            return tf.reduce_mean( indifference_logprob )\n",
    "\n",
    "        likelihood_i = tf.cond(max_idx >= 0,\n",
    "            lambda: true_fn(max_idx, f_samples, diff_mat),\n",
    "            lambda: false_fn(f_samples, mask_mat, diff_mat))\n",
    "\n",
    "        likelihood = likelihood + likelihood_i\n",
    "\n",
    "        return i+1, likelihood\n",
    "\n",
    "    cond = lambda i, _: i < num_data\n",
    "\n",
    "    _, likelihood = tf.while_loop(\n",
    "            cond,\n",
    "            body,\n",
    "            (0, tf.constant(0.0, dtype=tf.float64)),\n",
    "            parallel_iterations=10)\n",
    "\n",
    "    elbo = likelihood - klterm\n",
    "\n",
    "    return elbo\n",
    "\n",
    "\n",
    "def cholesky_matrix_inverse(A):\n",
    "    \"\"\"\n",
    "    :param A: Symmetric positive-definite matrix, tensor of shape (n, n)\n",
    "    :return: Inverse of A, tensor of shape (n, n)\n",
    "    \"\"\"\n",
    "    L = tf.linalg.cholesky(A)\n",
    "    L_inv = tf.linalg.triangular_solve(L, tf.eye(A.shape[0], dtype=tf.float64))\n",
    "    return tf.linalg.matrix_transpose(L_inv) @ L_inv\n",
    "\n",
    "\n",
    "def p_f_given_u(inducing_vars, inducing_inputs, kernel, inputs, invKmm_prior):\n",
    "    \"\"\"\n",
    "    Calculates the mean and covariance of the joint distriubtion p(f|u)\n",
    "    :param inducing_vars: tensor with shape (nsample,num_inducing,1)\n",
    "    :param inducing_inputs: tensor with shape (num_inducing, input_dims)\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: (tensor of shape (nsample,num_inputs), tensor of shape (num_inputs, num_inputs))\n",
    "    \"\"\"\n",
    "    Knm = kernel.K(inputs, inducing_inputs)  # (n, m)\n",
    "    A = Knm @ invKmm_prior  # (n, m)\n",
    "\n",
    "    f_mean = tf.squeeze(A @ inducing_vars, axis=-1)\n",
    "    # (nsample, num_inducing)\n",
    "\n",
    "    Knn = kernel.K(inputs)\n",
    "    f_cov = Knn - A @ tf.transpose(Knm)\n",
    "    # (num_inputs, num_inputs)\n",
    "\n",
    "    return f_mean, f_cov\n",
    "\n",
    "\n",
    "def q_f(q_mu, q_sqrt_latent, inducing_variables, kernel, inputs):\n",
    "    \"\"\"\n",
    "    Calculates the mean and covariance of the joint distribution q(f)\n",
    "    :param q_mu: tensor with shape (num_inducing, 1)\n",
    "    :param q_sqrt_latent: tensor with shape (1, num_inducing, num_inducing). Will be forced into lower triangular\n",
    "        matrix such that q_sqrt @ q_sqrt^T represents the covariance matrix of inducing variables\n",
    "    :param inducing_variables: tensor with shape (num_inducing, input_dims)\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: (tensor of shape (num_inputs), tensor of shape (num_inputs, num_inputs))\n",
    "    \"\"\"\n",
    "\n",
    "    q_sqrt = tf.linalg.band_part(q_sqrt_latent, -1, 0)  # Force into lower triangular\n",
    "    q_full = q_sqrt @ tf.linalg.matrix_transpose(q_sqrt)  # (1, num_data, num_data)\n",
    "\n",
    "    Kmm = kernel.K(inducing_variables)  # (m, m)\n",
    "    Kmm_inv = cholesky_matrix_inverse(Kmm)\n",
    "\n",
    "    Knm = kernel.K(inputs, inducing_variables)  # (n, m)\n",
    "    A = Knm @ Kmm_inv  # (n, m)\n",
    "\n",
    "    f_mean = tf.squeeze(A @ q_mu, axis=-1)\n",
    "\n",
    "    Knn = kernel.K(inputs)\n",
    "    S = tf.squeeze(q_full, axis=0)\n",
    "    f_cov = Knn + (A @ (S - Kmm) @ tf.linalg.matrix_transpose(A))\n",
    "\n",
    "    return f_mean, f_cov\n",
    "\n",
    "\n",
    "def populate_dicts(D_vals):\n",
    "    \"\"\"\n",
    "    Populates dictionaries to assign an index to each value seen in the training data.\n",
    "    :param D_vals: [k] list of 2-d ndarray [:,d] (to allow different num_choice for different observations)\n",
    "    \"\"\"\n",
    "    idx_to_val_dict = {}\n",
    "    val_to_idx_dict = {}\n",
    "\n",
    "    D_vals_list = np.concatenate(D_vals, axis=0)\n",
    "    D_vals_list_tuples = [tuple(i) for i in D_vals_list]\n",
    "    D_vals_set = set(D_vals_list_tuples)\n",
    "\n",
    "    for val in D_vals_set:\n",
    "        val_to_idx_dict[val] = len(val_to_idx_dict)\n",
    "        idx_to_val_dict[len(val_to_idx_dict)-1] = val\n",
    "\n",
    "    return idx_to_val_dict, val_to_idx_dict\n",
    "\n",
    "\n",
    "def val_to_idx(D_vals, max_vals, val_to_idx_dict):\n",
    "    \"\"\"\n",
    "    Converts training data from real values to index format using dictionaries.\n",
    "    Returns D_idxs (tensor with shape (k, num_choices, 1))\n",
    "        and max_idxs (tensor with shape (k, 1)):\n",
    "            max_idxs[i,0] is argmax of D_idxs[i,:,0]\n",
    "    :param D_vals: [k] list of ndarray [:,d]\n",
    "    :param max_vals: [k] list of ndarray [1,d]\n",
    "    \"\"\"\n",
    "\n",
    "    k = len(D_vals)\n",
    "\n",
    "    max_idxs = np.zeros(k, dtype=np.int32)\n",
    "\n",
    "    for i in range(k):\n",
    "        if max_vals[i] is not None:\n",
    "            diff = np.sum(np.square(D_vals[i] - max_vals[i]), axis=1)\n",
    "            max_idxs[i] = np.where(diff < 1e-30)[0]\n",
    "        else:\n",
    "            max_idxs[i] = -1\n",
    "\n",
    "    max_idxs = tf.constant(max_idxs)\n",
    "\n",
    "    D_idxs = tf.TensorArray(dtype=tf.int32, size=k, name='D_idxs', infer_shape=False, clear_after_read=False)\n",
    "\n",
    "    for i in range(k):\n",
    "        np.stack([ [val_to_idx_dict[tuple(datum)]] for datum in D_vals[i] ])\n",
    "\n",
    "    cond = lambda i, _: i < k\n",
    "    body = lambda i, D_idxs: \\\n",
    "        (i+1,\n",
    "         D_idxs.write(\n",
    "            i,\n",
    "            tf.constant([ val_to_idx_dict[tuple(datum)] for datum in D_vals[i] ], dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    _, D_idxs = tf.while_loop(cond, body, (0, D_idxs))\n",
    "\n",
    "    return D_idxs, max_idxs\n",
    "\n",
    "\n",
    "def train_model_fullcov(X,\n",
    "                        y,\n",
    "                        num_inducing,\n",
    "                        obj_low,\n",
    "                        obj_high,\n",
    "                        lengthscale=1.,\n",
    "                        num_steps=5000,\n",
    "                        indifference_threshold=None):\n",
    "    \"\"\"\n",
    "    if indifference_threshold is None:\n",
    "        indifference_threshold is trained with maximum likelihood estimation\n",
    "    else:\n",
    "        indifference_threshold is fixed\n",
    "    :param X: np array with shape (num_data, num_choices, input_dims). Ordinal data\n",
    "    :param y: np array with shape (num_data, input_dims). Most preferred input for each set of inputs. Each y value must\n",
    "    match exactly to one of the choices in its corresponding X entry\n",
    "    :param num_inducing: number of inducing variables to use\n",
    "    :param obj_low: int. Floor of possible inducing point value in each dimension\n",
    "    :param obj_high: int. Floor of possible inducing point value in each dimension\n",
    "    :param lengthscale: float. Lengthscale to initialize RBF kernel with\n",
    "    :param num_steps: int that specifies how many optimization steps to take when training model\n",
    "    :param indifference_threshold:\n",
    "    \"\"\"\n",
    "    input_dims = X.shape[2]\n",
    "    idx_to_val_dict, val_to_idx_dict = populate_dicts(X)\n",
    "    D_idxs, max_idxs = val_to_idx(X, y, val_to_idx_dict)\n",
    "\n",
    "    n = len(val_to_idx_dict.keys())\n",
    "    inputs = np.array([idx_to_val_dict[i] for i in range(n)])\n",
    "\n",
    "    # Initialize variational parameters\n",
    "\n",
    "    q_mu = tf.Variable(np.zeros([num_inducing, 1]), name=\"q_mu\", dtype=tf.float64)\n",
    "    q_sqrt_latent = tf.Variable(np.expand_dims(np.eye(num_inducing), axis=0), name=\"q_sqrt_latent\", dtype=tf.float64)\n",
    "    kernel = gpflow.kernels.RBF(lengthscale=[lengthscale for i in range(input_dims)])\n",
    "    kernel.lengthscale.transform = gpflow.utilities.bijectors.positive(lower=gpflow.default_jitter())\n",
    "    u = tf.Variable(np.array([[0.18024599],\n",
    "         [0.64495305],\n",
    "         [0.3011193 ],\n",
    "         [0.100001        ]]),\n",
    "                    name=\"u\",\n",
    "                    dtype=tf.float64,\n",
    "                    constraint=lambda x: tf.clip_by_value(x, obj_low, obj_high))\n",
    "\n",
    "    is_threshold_trainable = (indifference_threshold is None)\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        indifference_threshold = tf.Variable(0.1, dtype=tf.float64,\n",
    "                        constraint=lambda x: tf.clip_by_value(x,\n",
    "                                                clip_value_min=0.0,\n",
    "                                                clip_value_max=np.infty))\n",
    "\n",
    "    neg_elbo = lambda: -elbo_fullcov(q_mu=q_mu,\n",
    "                                     q_sqrt_latent=q_sqrt_latent,\n",
    "                                     inducing_inputs=u,\n",
    "                                     D_idxs=D_idxs,\n",
    "                                     max_idxs=max_idxs,\n",
    "                                     kernel=kernel,\n",
    "                                     inputs=inputs,\n",
    "                                     indifference_threshold=indifference_threshold,\n",
    "                                     n_inducing_sample=50,\n",
    "                                     n_f_given_inducing_sample=50)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        print(\"Indifference_threshold is trainable.\")\n",
    "        trainable_vars = [q_mu, q_sqrt_latent, u, indifference_threshold] + list(kernel.trainable_variables)\n",
    "    else:\n",
    "        print(\"Indifference_threshold is fixed at {}\".format(indifference_threshold))\n",
    "        trainable_vars = [q_mu, q_sqrt_latent, u] + list(kernel.trainable_variables)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    vals = []\n",
    "\n",
    "    try:\n",
    "        for i in range(num_steps):\n",
    "            vals.append((q_mu.numpy(), q_sqrt_latent.numpy(), u.numpy(), inputs, kernel.lengthscale.numpy(), kernel.variance.numpy(), neg_elbo().numpy()))\n",
    "            print(i)\n",
    "            optimizer.minimize(neg_elbo, var_list=trainable_vars)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print('Negative ELBO at step {}: {} in {:.4f}s'.format(i,\n",
    "                           neg_elbo().numpy(),\n",
    "                           time.time() - start_time))\n",
    "\n",
    "                start_time = time.time()\n",
    "    except tf.errors.InvalidArgumentError as err:\n",
    "        print(err)\n",
    "        print(q_mu)\n",
    "        print(q_sqrt_latent)\n",
    "        print(u)\n",
    "        print(inputs)\n",
    "        gpflow.utilities.print_summary(kernel)\n",
    "        return vals\n",
    "        \n",
    "    return vals\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        return q_mu, tf.linalg.band_part(q_sqrt_latent, -1, 0), u, inputs, kernel, indifference_threshold  # q_mu and q_sqrt\n",
    "    else:\n",
    "        return q_mu, tf.linalg.band_part(q_sqrt_latent, -1, 0), u, inputs, kernel  # q_mu and q_sqrt\n",
    "\n",
    "\n",
    "def init_SVGP_fullcov(q_mu, q_sqrt, inducing_variables, kernel, likelihood):\n",
    "    \"\"\"\n",
    "    Returns a gpflow SVGP model using the values obtained from train_model.\n",
    "    :param q_mu: np array or tensor of shape (num_inputs, 1)\n",
    "    :param q_sqrt: np array or tensor of shape (num_inputs, num_inputs). Lower triangular matrix\n",
    "    :param inducing_variables: tensor of shape (num_inducing, input_dims)\n",
    "    :param inputs: np array or tensor of shape (num_inputs, input_dims)\n",
    "    :param kernel: gpflow kernel\n",
    "    :param likelihood: gpflow likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    model = gpflow.models.SVGP(kernel=kernel,\n",
    "                               likelihood=likelihood,\n",
    "                               inducing_variable=inducing_variables,\n",
    "                               whiten=False)\n",
    "\n",
    "    model.q_mu.assign(q_mu)\n",
    "    model.q_sqrt.assign(q_sqrt)\n",
    "\n",
    "    # Set so that the parameters learned do not change if further optimization over\n",
    "    # other parameters is performed\n",
    "    set_trainable(model.q_mu, False)\n",
    "    set_trainable(model.q_sqrt, False)\n",
    "    set_trainable(model.inducing_variable.Z, False)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((X_results, y_results, best_guess_results), open(results_dir + \"Xybestguess.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    \"\"\"\n",
    "    x and y have shape (..., input_dims)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x - y) * (x - y), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = uniform_grid(input_dims, num_discrete_per_dim, low=objective_low, high=objective_high)\n",
    "global_min = xx[np.argmin(objective(xx))][0]\n",
    "\n",
    "for i in range(best_guess_results.shape[0]):\n",
    "    diff_from_min = dist(best_guess_results[i], global_min)\n",
    "    \n",
    "    x_axis = list(range(num_combs+1, num_combs+1+num_evals))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_axis, diff_from_min, 'kx', mew=2)\n",
    "    plt.xticks(x_axis)\n",
    "    plt.xlabel('Evaluations', fontsize=18)\n",
    "    plt.ylabel('Best guess distance', fontsize=16)\n",
    "    plt.title(\"Run %s\" % i)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
