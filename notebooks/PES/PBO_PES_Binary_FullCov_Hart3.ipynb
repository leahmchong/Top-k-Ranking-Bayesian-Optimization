{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preferential Bayesian Optimization: Predictive Entropy Search\n",
    "This notebook demonstrates the use of the Predictive Entropy Search (PES) acquisition function on ordinal (preference) data. \n",
    "\n",
    "Formulation by Nguyen Quoc Phong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from gpflow.utilities import set_trainable, print_summary\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.split(os.path.split(os.path.split(os.getcwd())[0])[0])[0]) # Move 3 levels up directory to import PBO\n",
    "import PBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = PBO.objectives.hartmann3d\n",
    "objective_low = 0\n",
    "objective_high = 1.\n",
    "objective_name = \"Hart3\"\n",
    "acquisition_name = \"PES\"\n",
    "experiment_name = \"PBO\" + \"_\" + acquisition_name + \"_\" + objective_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 20\n",
    "num_evals = 20\n",
    "num_samples = 100\n",
    "num_choices = 2\n",
    "input_dims = 3\n",
    "num_maximizers = 20\n",
    "num_init_points = 3\n",
    "num_inducing_init = 3\n",
    "num_discrete_per_dim = 100 # Discretization of continuous input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /home/sebtsh/PBO/notebooks/PES/results/PBO_PES_Hart3/  already exists\n"
     ]
    }
   ],
   "source": [
    "results_dir = os.getcwd() + '/results/' + experiment_name + '/'\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.makedirs(results_dir)\n",
    "    print(\"Directory \" , results_dir ,  \" created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , results_dir ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_observation(X, objective):\n",
    "    f = PBO.objectives.objective_get_f_neg(X, objective)\n",
    "    return PBO.observation_model.gen_observation_from_f(X, f, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_visualize(X, y, num_inducing, title):\n",
    "    \n",
    "    # Train model with data\n",
    "    q_mu, q_sqrt, u, inputs, k, indifference_threshold = PBO.models.learning_stochastic.train_model_fullcov(X, y, \n",
    "                                                                         num_inducing=num_inducing,\n",
    "                                                                         obj_low=objective_low,\n",
    "                                                                         obj_high=objective_high,\n",
    "                                                                         num_steps=3000)\n",
    "    likelihood = gpflow.likelihoods.Gaussian()\n",
    "    model = PBO.models.learning.init_SVGP_fullcov(q_mu, q_sqrt, u, k, likelihood)\n",
    "    u_mean = q_mu.numpy()\n",
    "    inducing_vars = u.numpy()\n",
    "    \n",
    "    return model, inputs, u_mean, inducing_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_grid(input_dims, num_discrete_per_dim, low=0., high=1.):\n",
    "    \"\"\"\n",
    "    Returns an array with all possible permutations of discrete values in input_dims number of dimensions.\n",
    "    :param input_dims: int\n",
    "    :param num_discrete_per_dim: int\n",
    "    :param low: int\n",
    "    :param high: int\n",
    "    :return: tensor of shape (num_discrete_per_dim ** input_dims, input_dims)\n",
    "    \"\"\"\n",
    "    num_points = num_discrete_per_dim ** input_dims\n",
    "    out = np.zeros([num_points, input_dims])\n",
    "    discrete_points = np.linspace(low, high, num_discrete_per_dim)\n",
    "    for i in range(num_points):\n",
    "        for dim in range(input_dims):\n",
    "            val = num_discrete_per_dim ** (dim)\n",
    "            out[i, dim] = discrete_points[int((i // val) % num_discrete_per_dim)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is our main metric for the performance of the acquisition function: The closer the model's best guess to the global minimum, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_guess(model):\n",
    "    \"\"\"\n",
    "    Returns a GP model's best guess of the global maximum of f.\n",
    "    \"\"\"\n",
    "    xx = uniform_grid(input_dims, num_discrete_per_dim, low=objective_low, high=objective_high)\n",
    "    res = model.predict_f(xx)[0].numpy()\n",
    "    return xx[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in these arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_at_end = int((num_init_points-1) * num_init_points / 2 + num_evals)\n",
    "X_results = np.zeros([num_runs, num_data_at_end, num_choices, input_dims])\n",
    "y_results = np.zeros([num_runs, num_data_at_end, input_dims])\n",
    "best_guess_results = np.zeros([num_runs, num_evals, input_dims])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the initial values for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "init_points = np.random.uniform(size=[num_runs, num_init_points, input_dims], low=objective_low, high=objective_high)\n",
    "num_combs = int((num_init_points-1) * num_init_points / 2)\n",
    "init_vals = np.zeros([num_runs, num_combs, num_choices, input_dims])\n",
    "for run in range(num_runs):\n",
    "    cur_idx = 0\n",
    "    for init_point in range(num_init_points-1):\n",
    "        for next_point in range(init_point+1, num_init_points):\n",
    "            init_vals[run, cur_idx, 0] = init_points[run, init_point]\n",
    "            init_vals[run, cur_idx, 1] = init_points[run, next_point]\n",
    "            cur_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loops carry out the Bayesian optimization algorithm over a number of runs, with a fixed number of evaluations per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning run 0\n",
      "Indifference_threshold is trainable.\n",
      "WARNING:tensorflow:From /home/sebtsh/anaconda3/envs/gpflow/lib/python3.6/site-packages/tensorflow_core/python/ops/array_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "Negative ELBO at step 0: 5.299011499102869 in 0.1823s\n",
      "Negative ELBO at step 500: 2.27732341431458 in 40.9868s\n",
      "Negative ELBO at step 1000: 2.2166783796509617 in 40.7079s\n",
      "Negative ELBO at step 1500: 2.219928344501303 in 40.2445s\n",
      "Negative ELBO at step 2000: 2.072828913056528 in 40.4824s\n",
      "Negative ELBO at step 2500: 2.052806393252899 in 40.2100s\n",
      "Beginning evaluation 0\n",
      "Evaluation 0: Sampling maximizers\n",
      "WARNING:tensorflow:From /home/sebtsh/anaconda3/envs/gpflow/lib/python3.6/site-packages/tensorflow_probability/python/distributions/distribution.py:284: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.\n",
      "Instructions for updating:\n",
      "`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.\n",
      "Loss at step 0: -78.08200656823897\n",
      "Loss at step 500: -235.48673224849514\n",
      "Loss at step 1000: -268.79553378570205\n",
      "Loss at step 1500: -271.5955370503516\n",
      "Loss at step 2000: -272.9823571573774\n",
      "Loss at step 2077: -273.1065364313307\n",
      "tf.Tensor(\n",
      "[[0.         1.         0.95742495]\n",
      " [1.         1.         0.60039371]\n",
      " [1.         0.         0.85989686]\n",
      " [1.         0.09724292 1.        ]\n",
      " [1.         0.         1.        ]\n",
      " [0.21745432 0.50012038 0.2574017 ]\n",
      " [0.57540653 1.         0.71407644]\n",
      " [0.7166398  1.         1.        ]\n",
      " [0.68706075 1.         0.        ]\n",
      " [1.         0.67796334 1.        ]\n",
      " [1.         0.24862996 0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         1.         0.69255661]\n",
      " [0.37630409 0.         0.42009116]\n",
      " [0.95067171 1.         0.        ]\n",
      " [1.         0.         1.        ]\n",
      " [0.30101525 0.43390224 0.34060473]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         0.93121664]\n",
      " [0.15537025 0.26363152 0.98141581]], shape=(20, 3), dtype=float64)\n",
      "Evaluation 0: Calculating I\n",
      "Evaluation 0: Next query is tf.Tensor(\n",
      "[[0.43758721 0.891773   0.96366276]\n",
      " [0.82865691 0.92496691 0.04600731]], shape=(2, 3), dtype=float64) with I value of 0.048991552198823486\n",
      "Evaluation 0: Training model\n",
      "Indifference_threshold is trainable.\n",
      "Negative ELBO at step 0: 51.163308759968785 in 0.1855s\n",
      "Negative ELBO at step 500: 3.8946516551795547 in 49.2281s\n",
      "Negative ELBO at step 1000: 3.1524045013768647 in 48.8535s\n",
      "Negative ELBO at step 1500: 2.9580694257528424 in 49.6692s\n",
      "Negative ELBO at step 2000: 2.8241169163109783 in 49.2999s\n",
      "Negative ELBO at step 2500: 2.8708758691827136 in 50.6176s\n",
      "Beginning evaluation 1\n",
      "Evaluation 1: Sampling maximizers\n",
      "Loss at step 0: -28.3087090745629\n",
      "Loss at step 500: -229.33671413468463\n",
      "Loss at step 1000: -261.94442863769405\n",
      "Loss at step 1500: -264.8391841372969\n",
      "Loss at step 2000: -266.2234888628908\n",
      "Loss at step 2274: -266.51969208423253\n",
      "tf.Tensor(\n",
      "[[0.91090039 1.         0.86196852]\n",
      " [1.         0.46539312 0.70748789]\n",
      " [0.80899195 0.8152492  0.99386711]\n",
      " [1.         0.68129115 0.87460494]\n",
      " [1.         1.         1.        ]\n",
      " [1.         0.02063445 0.59666516]\n",
      " [0.         0.3703729  0.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.         0.19402296 0.13299071]\n",
      " [1.         0.89471222 1.        ]\n",
      " [1.         1.         0.25760262]\n",
      " [1.         0.97286328 0.        ]\n",
      " [0.90858454 0.01069191 0.77964562]\n",
      " [0.37481625 0.75289865 1.        ]\n",
      " [1.         0.         0.66569216]\n",
      " [1.         1.         1.        ]\n",
      " [0.         0.45181646 0.03752493]\n",
      " [1.         1.         0.64712187]\n",
      " [0.38335357 0.42420182 1.        ]\n",
      " [1.         1.         0.85124698]], shape=(20, 3), dtype=float64)\n",
      "Evaluation 1: Calculating I\n",
      "Evaluation 1: Next query is tf.Tensor(\n",
      "[[0.82865691 0.92496691 0.04600731]\n",
      " [0.92721181 0.02895255 0.89569129]], shape=(2, 3), dtype=float64) with I value of 0.05914706178924112\n",
      "Evaluation 1: Training model\n",
      "Indifference_threshold is trainable.\n",
      "Negative ELBO at step 0: 20.647466347934248 in 0.2018s\n",
      "Negative ELBO at step 500: 4.390801687050959 in 58.3069s\n",
      "Negative ELBO at step 1000: 3.907111583749414 in 58.9010s\n",
      "Negative ELBO at step 1500: 3.8474227327164146 in 58.3741s\n",
      "Negative ELBO at step 2000: 3.7437007395251674 in 58.7770s\n",
      "Negative ELBO at step 2500: 3.6995459912350084 in 57.8725s\n",
      "Beginning evaluation 2\n",
      "Evaluation 2: Sampling maximizers\n",
      "Loss at step 0: 20.450419210062712\n",
      "Loss at step 500: -135.93386366648178\n",
      "Loss at step 1000: -163.4055754399495\n",
      "Loss at step 1500: -164.85468401706922\n",
      "Loss at step 2000: -165.29700872099937\n",
      "Loss at step 2500: -165.42261038549822\n",
      "tf.Tensor(\n",
      "[[0.62263888 0.08630675 0.        ]\n",
      " [0.6901997  1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.84678852 0.         0.09344619]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         0.91705874]\n",
      " [1.         1.         0.361407  ]\n",
      " [0.80730407 1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.         1.         0.91687779]\n",
      " [0.         0.63312298 0.68375935]\n",
      " [1.         1.         0.1950502 ]\n",
      " [1.         0.94540483 1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.         0.67622787 0.393316  ]\n",
      " [1.         0.3750752  0.5744116 ]\n",
      " [0.06174372 0.         1.        ]\n",
      " [1.         1.         1.        ]], shape=(20, 3), dtype=float64)\n",
      "Evaluation 2: Calculating I\n",
      "Evaluation 2: Next query is tf.Tensor(\n",
      "[[0.82865691 0.92496691 0.04600731]\n",
      " [0.00531005 0.01135513 0.51122179]], shape=(2, 3), dtype=float64) with I value of 0.04687699145790479\n",
      "Evaluation 2: Training model\n",
      "Indifference_threshold is trainable.\n",
      "Negative ELBO at step 0: 320.1179997024093 in 0.2551s\n",
      "Negative ELBO at step 500: 10.096771233731527 in 67.0987s\n",
      "Negative ELBO at step 1000: 6.432612223612964 in 67.7792s\n",
      "Negative ELBO at step 1500: 5.232546492812163 in 66.7219s\n",
      "Negative ELBO at step 2000: 4.842515501293964 in 65.5949s\n",
      "Negative ELBO at step 2500: 4.8389161872334014 in 67.1114s\n",
      "Beginning evaluation 3\n",
      "Evaluation 3: Sampling maximizers\n",
      "Loss at step 0: -41.826968897891966\n",
      "Loss at step 500: -256.70979567840817\n",
      "Loss at step 1000: -298.16484275816117\n",
      "Loss at step 1500: -301.07408626255756\n",
      "Loss at step 2000: -302.08133700204894\n",
      "Loss at step 2500: -302.2145720627242\n",
      "tf.Tensor(\n",
      "[[0.20100502 0.25445028 1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         0.         0.1637847 ]\n",
      " [0.         0.72270778 0.        ]\n",
      " [1.         1.         0.3378054 ]\n",
      " [0.35301978 0.         0.43353589]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         0.24257813]\n",
      " [0.25449425 0.         1.        ]\n",
      " [1.         0.54836782 0.73911995]\n",
      " [0.         1.         0.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.         0.72400492 1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         0.10235007 0.        ]\n",
      " [0.03806452 0.41140999 0.96095786]\n",
      " [1.         0.54071048 0.75359525]\n",
      " [0.0827239  0.         0.68209476]\n",
      " [0.         0.73360022 1.        ]], shape=(20, 3), dtype=float64)\n",
      "Evaluation 3: Calculating I\n",
      "Evaluation 3: Next query is tf.Tensor(\n",
      "[[0.43758721 0.891773   0.96366276]\n",
      " [0.96081253 0.06512147 0.04457111]], shape=(2, 3), dtype=float64) with I value of 0.06855242562092183\n",
      "Evaluation 3: Training model\n",
      "Indifference_threshold is trainable.\n",
      "Negative ELBO at step 0: 241.26792735745332 in 0.2412s\n",
      "Negative ELBO at step 500: 12.598805566025277 in 74.3142s\n",
      "Negative ELBO at step 1000: 6.192397166967242 in 75.1895s\n",
      "Negative ELBO at step 1500: 5.173546303198086 in 74.2664s\n",
      "Negative ELBO at step 2000: 5.047235718696923 in 75.7460s\n",
      "Negative ELBO at step 2500: 4.829356282080598 in 77.0880s\n",
      "Beginning evaluation 4\n",
      "Evaluation 4: Sampling maximizers\n",
      "Loss at step 0: -14.862177306097045\n",
      "Loss at step 500: -234.83401238276684\n",
      "Loss at step 1000: -275.8510948167189\n",
      "Loss at step 1500: -282.3067277851361\n"
     ]
    }
   ],
   "source": [
    "for run in range(num_runs):\n",
    "    print(\"Beginning run %s\" % (run))\n",
    "    \n",
    "    X = init_vals[run]\n",
    "    y = get_noisy_observation(X, objective)\n",
    "    \n",
    "    model, inputs, u_mean, inducing_vars = train_and_visualize(X, y, num_inducing_init, \"Run_{}:_Initial_model\".format(run))\n",
    "\n",
    "    for evaluation in range(num_evals):\n",
    "        print(\"Beginning evaluation %s\" % (evaluation)) \n",
    "\n",
    "        # Sample possible next queries\n",
    "        \n",
    "        samples = PBO.acquisitions.pes.sample_inputs(inputs, num_samples, num_choices)\n",
    "\n",
    "        # Sample maximizers\n",
    "        print(\"Evaluation %s: Sampling maximizers\" % (evaluation))\n",
    "        maximizers = PBO.fourier_features.sample_maximizers(X=inducing_vars,\n",
    "                                                            count=num_maximizers,\n",
    "                                                            n_init=10,\n",
    "                                                            D=100,\n",
    "                                                            model=model)\n",
    "        print(maximizers)\n",
    "\n",
    "        # Calculate PES value I for each possible next query\n",
    "        print(\"Evaluation %s: Calculating I\" % (evaluation))\n",
    "        I_vals = PBO.acquisitions.pes.I_batch(samples, maximizers, model)\n",
    "\n",
    "        # Select query that maximizes I\n",
    "        next_idx = np.argmax(I_vals)\n",
    "        next_query = samples[next_idx]\n",
    "        print(\"Evaluation %s: Next query is %s with I value of %s\" % (evaluation, next_query, I_vals[next_idx]))\n",
    "\n",
    "        X = np.concatenate([X, [next_query]])\n",
    "        # Evaluate objective function\n",
    "        y = get_noisy_observation(X, objective)\n",
    "        \n",
    "        print(\"Evaluation %s: Training model\" % (evaluation))\n",
    "        model, inputs, u_mean, inducing_vars = train_and_visualize(X, y, \n",
    "                                                                   num_inducing_init + evaluation + 1, \n",
    "                                                                   \"Run_{}_Evaluation_{}\".format(run, evaluation))\n",
    "\n",
    "        best_guess_results[run, evaluation, :] = best_guess(model)\n",
    "        \n",
    "        pickle.dump((model.kernel.variance.numpy(), \n",
    "                     model.kernel.lengthscale.numpy(),\n",
    "                     model.inducing_variable.Z.numpy(), \n",
    "                     model.q_mu.numpy(), \n",
    "                     model.q_sqrt.numpy()), \n",
    "                    open(results_dir + \"Run_{}_Evaluation_{}_model.p\".format(run, evaluation), \"wb\"))\n",
    "\n",
    "    X_results[run] = X\n",
    "    y_results[run] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((X_results, y_results, best_guess_results), open(results_dir + \"Xybestguess.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    \"\"\"\n",
    "    x and y have shape (..., input_dims)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x - y) * (x - y), axis=-1))\n",
    "\n",
    "xx = uniform_grid(input_dims, num_discrete_per_dim, low=objective_low, high=objective_high)\n",
    "global_min = xx[np.argmin(objective(xx))][0]\n",
    "\n",
    "for i in range(best_guess_results.shape[0]):\n",
    "    diff_from_min = dist(best_guess_results[i], global_min)\n",
    "    \n",
    "    x_axis = list(range(num_combs+1, num_combs+1+num_evals))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_axis, diff_from_min, 'kx', mew=2)\n",
    "    plt.xticks(x_axis)\n",
    "    plt.xlabel('Evaluations', fontsize=18)\n",
    "    plt.ylabel('Best guess distance', fontsize=16)\n",
    "    plt.title(\"Run %s\" % i)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
