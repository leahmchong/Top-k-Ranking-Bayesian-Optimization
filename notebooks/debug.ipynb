{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from gpflow.utilities import set_trainable, print_summary\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.split(os.path.split(os.getcwd())[0])[0]) # Move 3 levels up directory to import PBO\n",
    "import PBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_stochastic.py. Changed train_model_fullcov to take pre-determined inducing inputs for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#     1. use samples to compute the objective function\n",
    "#         elbo_fullcov(q_mu, q_sqrt_latent, inducing_variables, D_idxs, max_idxs, kernel, inputs)\n",
    "#        then it can works for different number of choices\n",
    "#     2. val_to_idxs, populate_dicts: need to adaptive to different number of choices\n",
    "#         by converting some to list\n",
    "#     2. check function q_f to see if the implementation is correct\n",
    "#         checked: correct\n",
    "\"\"\"\n",
    "    forester_get_Y:\n",
    "        as X is a list, change the function!\n",
    "    sample maximizers:\n",
    "        change observations from inducing input, inducing variables\n",
    "        to distribution of inducing variables\n",
    "\n",
    "Given ordinal (preference) data consisting of sets of input points and a most preferred input point for every such set,\n",
    "the train_model function learns variational parameters that approximate the distribution of a latent function f over\n",
    "all input points present in the data, which can be used to construct GP models to approximate f over the entire input\n",
    "space.\n",
    "Formulation by Nguyen Quoc Phong.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import gpflow\n",
    "from gpflow.utilities import set_trainable\n",
    "\n",
    "\n",
    "def elbo_fullcov(q_mu,\n",
    "                q_sqrt_latent,\n",
    "                inducing_inputs,\n",
    "                D_idxs,\n",
    "                max_idxs,\n",
    "                kernel,\n",
    "                inputs,\n",
    "                indifference_threshold,\n",
    "                n_inducing_sample=50,\n",
    "                n_f_given_inducing_sample=30):\n",
    "    \"\"\"\n",
    "    Calculates the ELBO for the PBO formulation, using a full covariance matrix.\n",
    "    :param q_mu: tensor with shape (num_inducing, 1)\n",
    "    :param q_sqrt_latent: tensor with shape (1, num_inducing, num_inducing). Will be forced into lower triangular\n",
    "        matrix such that q_sqrt @ q_sqrt^T represents the covariance matrix of inducing variables\n",
    "    :param inducing_inputs: tensor with shape (num_inducing, input_dims)\n",
    "    :param D_idxs: tensor with shape (num_data, num_choices, 1)\n",
    "        Input data points, that are indices into q_mu and q_var for tf.gather_nd\n",
    "    :param max_idxs: tensor with shape (num_data, 1)\n",
    "        Selection of most preferred input point for each collection of data points, that are indices into\n",
    "        q_mu and q_var\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: tensor of shape ()\n",
    "    \"\"\"\n",
    "    Kmm = kernel.K(inducing_inputs)\n",
    "\n",
    "    logdet_Kmm = tf.linalg.logdet(Kmm)\n",
    "    invKmm = cholesky_matrix_inverse(Kmm)\n",
    "\n",
    "    num_data = D_idxs.size()\n",
    "\n",
    "    # 1. Sample from q(u)\n",
    "    standard_mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=tf.zeros(tf.shape(q_mu)[0], dtype=tf.float64),\n",
    "            scale_diag=tf.ones(tf.shape(q_mu)[0], dtype=tf.float64))\n",
    "\n",
    "    standard_mvn_samples = standard_mvn.sample(n_inducing_sample)\n",
    "    # (n_inducing_sample, num_inducing)\n",
    "\n",
    "    q_sqrt = tf.linalg.band_part(q_sqrt_latent, -1, 0)  # Force into lower triangular\n",
    "    q_full = q_sqrt @ tf.linalg.matrix_transpose(q_sqrt)  # (1, num_data, num_data)\n",
    "    inv_q_full = cholesky_matrix_inverse(tf.squeeze(q_full, axis=0))\n",
    "\n",
    "    posterior_inducing_samples = q_sqrt @ tf.expand_dims(standard_mvn_samples, axis=-1) + q_mu\n",
    "    # (n_inducing_sample, num_inducing, 1)\n",
    "\n",
    "    # 2. p(f|u) where u are samples from q(u)\n",
    "    f_mean_given_inducing_sample, f_cov_given_inducing_sample = p_f_given_u(\n",
    "            posterior_inducing_samples,\n",
    "            inducing_inputs, kernel, inputs, invKmm)\n",
    "    # f_mean: (n_inducing_sample, num_inputs)\n",
    "    # f_cov: (num_inputs, num_inputs)\n",
    "\n",
    "    # 3. KL[q(u) || p(u)] = E_{q(u)} [log q(u)] - [log p(u)]\n",
    "    zero_mean_inducing_samples = posterior_inducing_samples - q_mu\n",
    "    # (n_inducing_sample, num_inducing, 1)\n",
    "\n",
    "    klterm = -0.5 * tf.reduce_mean(\n",
    "        tf.linalg.logdet(q_full)\n",
    "        + tf.transpose(zero_mean_inducing_samples, perm=[0,2,1]) @ inv_q_full @ zero_mean_inducing_samples\n",
    "        - logdet_Kmm\n",
    "        - tf.transpose(posterior_inducing_samples, perm= [0,2,1]) @ invKmm @ posterior_inducing_samples\n",
    "    )\n",
    "\n",
    "    def body(i, likelihood):\n",
    "        idxs = tf.squeeze(D_idxs.read(i))\n",
    "        max_idx = max_idxs[i]\n",
    "        num_choice = tf.shape(idxs)[0]\n",
    "\n",
    "        fi_cov = tf.gather(\n",
    "            tf.gather(f_cov_given_inducing_sample, indices=idxs, axis=0),\n",
    "            indices=idxs, axis=1)\n",
    "        # (num_choice, num_choice)\n",
    "\n",
    "        fi_mean = tf.gather(f_mean_given_inducing_sample, indices=idxs, axis=1)\n",
    "        # (n_inducing_sample, num_choice)\n",
    "\n",
    "        standard_mvn_i = tfp.distributions.MultivariateNormalDiag(\n",
    "                loc=tf.zeros_like(idxs, dtype=tf.float64),\n",
    "                scale_diag=tf.ones_like(idxs, dtype=tf.float64))\n",
    "\n",
    "        standard_mvn_i_samples = standard_mvn_i.sample(n_f_given_inducing_sample)\n",
    "        # (n_f_given_inducing_sample, num_choice)\n",
    "\n",
    "        eigvalues, eigvects = tf.linalg.eigh(fi_cov)\n",
    "        eigvalues = tf.clip_by_value(eigvalues, clip_value_min=0., clip_value_max=np.infty)\n",
    "        transform_mat = eigvects @ tf.linalg.diag(tf.sqrt(eigvalues))\n",
    "\n",
    "        zero_mean_f_samples = tf.squeeze(transform_mat @ tf.expand_dims(standard_mvn_i_samples, axis=-1), axis=-1)\n",
    "        # (n_f_given_inducing_sample, num_choice)\n",
    "\n",
    "        f_samples = tf.expand_dims(zero_mean_f_samples, axis=1) + fi_mean\n",
    "        # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "        # implementing a threshold for the choice of indifference\n",
    "        mask_mat = tf.eye(num_choice, dtype=tf.float64)\n",
    "        diff_mat = (1.0 - mask_mat) * indifference_threshold\n",
    "\n",
    "        def true_fn(max_idx, f_samples, diff_mat):\n",
    "            f_samples = f_samples + tf.gather(diff_mat, indices=max_idx, axis=0)\n",
    "\n",
    "            return tf.reduce_mean(\n",
    "                    tf.gather(f_samples, indices=max_idx, axis=-1)\n",
    "                    - tf.reduce_logsumexp(f_samples, axis=-1))\n",
    "\n",
    "        def false_fn(f_samples, mask_mat, diff_mat):\n",
    "            max_idx_f_samples = tf.squeeze(mask_mat @ tf.expand_dims(f_samples, axis=-1), axis=-1)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "            all_f_samples = tf.expand_dims(f_samples, axis=-1) + tf.gather(diff_mat, indices=max_idx, axis=0)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice, num_choice)\n",
    "\n",
    "            all_choice_logprob = max_idx_f_samples - tf.reduce_logsumexp(all_f_samples, axis=-2)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "            indifference_prob = 1.0 - tf.exp( tf.reduce_logsumexp(all_choice_logprob, axis=-1) )\n",
    "            indifference_prob = tf.clip_by_value(indifference_prob, clip_value_min=1e-50, clip_value_max=1.0 - 1e-50)\n",
    "            indifference_logprob = tf.math.log(indifference_prob)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample)\n",
    "\n",
    "            return tf.reduce_mean( indifference_logprob )\n",
    "\n",
    "        likelihood_i = tf.cond(max_idx >= 0,\n",
    "            lambda: true_fn(max_idx, f_samples, diff_mat),\n",
    "            lambda: false_fn(f_samples, mask_mat, diff_mat))\n",
    "\n",
    "        likelihood = likelihood + likelihood_i\n",
    "\n",
    "        return i+1, likelihood\n",
    "\n",
    "    cond = lambda i, _: i < num_data\n",
    "\n",
    "    _, likelihood = tf.while_loop(\n",
    "            cond,\n",
    "            body,\n",
    "            (0, tf.constant(0.0, dtype=tf.float64)),\n",
    "            parallel_iterations=10)\n",
    "\n",
    "    elbo = likelihood - klterm\n",
    "\n",
    "    return elbo\n",
    "\n",
    "\n",
    "def cholesky_matrix_inverse(A):\n",
    "    \"\"\"\n",
    "    :param A: Symmetric positive-definite matrix, tensor of shape (n, n)\n",
    "    :return: Inverse of A, tensor of shape (n, n)\n",
    "    \"\"\"\n",
    "    L = tf.linalg.cholesky(A)\n",
    "    L_inv = tf.linalg.triangular_solve(L, tf.eye(A.shape[0], dtype=tf.float64))\n",
    "    return tf.linalg.matrix_transpose(L_inv) @ L_inv\n",
    "\n",
    "\n",
    "def p_f_given_u(inducing_vars, inducing_inputs, kernel, inputs, invKmm_prior):\n",
    "    \"\"\"\n",
    "    Calculates the mean and covariance of the joint distriubtion p(f|u)\n",
    "    :param inducing_vars: tensor with shape (nsample,num_inducing,1)\n",
    "    :param inducing_inputs: tensor with shape (num_inducing, input_dims)\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: (tensor of shape (nsample,num_inputs), tensor of shape (num_inputs, num_inputs))\n",
    "    \"\"\"\n",
    "    Knm = kernel.K(inputs, inducing_inputs)  # (n, m)\n",
    "    A = Knm @ invKmm_prior  # (n, m)\n",
    "\n",
    "    f_mean = tf.squeeze(A @ inducing_vars, axis=-1)\n",
    "    # (nsample, num_inducing)\n",
    "\n",
    "    Knn = kernel.K(inputs)\n",
    "    f_cov = Knn - A @ tf.transpose(Knm)\n",
    "    # (num_inputs, num_inputs)\n",
    "\n",
    "    return f_mean, f_cov\n",
    "\n",
    "\n",
    "def q_f(q_mu, q_sqrt_latent, inducing_variables, kernel, inputs):\n",
    "    \"\"\"\n",
    "    Calculates the mean and covariance of the joint distribution q(f)\n",
    "    :param q_mu: tensor with shape (num_inducing, 1)\n",
    "    :param q_sqrt_latent: tensor with shape (1, num_inducing, num_inducing). Will be forced into lower triangular\n",
    "        matrix such that q_sqrt @ q_sqrt^T represents the covariance matrix of inducing variables\n",
    "    :param inducing_variables: tensor with shape (num_inducing, input_dims)\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: (tensor of shape (num_inputs), tensor of shape (num_inputs, num_inputs))\n",
    "    \"\"\"\n",
    "\n",
    "    q_sqrt = tf.linalg.band_part(q_sqrt_latent, -1, 0)  # Force into lower triangular\n",
    "    q_full = q_sqrt @ tf.linalg.matrix_transpose(q_sqrt)  # (1, num_data, num_data)\n",
    "\n",
    "    Kmm = kernel.K(inducing_variables)  # (m, m)\n",
    "    Kmm_inv = cholesky_matrix_inverse(Kmm)\n",
    "\n",
    "    Knm = kernel.K(inputs, inducing_variables)  # (n, m)\n",
    "    A = Knm @ Kmm_inv  # (n, m)\n",
    "\n",
    "    f_mean = tf.squeeze(A @ q_mu, axis=-1)\n",
    "\n",
    "    Knn = kernel.K(inputs)\n",
    "    S = tf.squeeze(q_full, axis=0)\n",
    "    f_cov = Knn + (A @ (S - Kmm) @ tf.linalg.matrix_transpose(A))\n",
    "\n",
    "    return f_mean, f_cov\n",
    "\n",
    "\n",
    "def populate_dicts(D_vals):\n",
    "    \"\"\"\n",
    "    Populates dictionaries to assign an index to each value seen in the training data.\n",
    "    :param D_vals: [k] list of 2-d ndarray [:,d] (to allow different num_choice for different observations)\n",
    "    \"\"\"\n",
    "    idx_to_val_dict = {}\n",
    "    val_to_idx_dict = {}\n",
    "\n",
    "    D_vals_list = np.concatenate(D_vals, axis=0)\n",
    "    D_vals_list_tuples = [tuple(i) for i in D_vals_list]\n",
    "    D_vals_set = set(D_vals_list_tuples)\n",
    "\n",
    "    for val in D_vals_set:\n",
    "        val_to_idx_dict[val] = len(val_to_idx_dict)\n",
    "        idx_to_val_dict[len(val_to_idx_dict)-1] = val\n",
    "\n",
    "    return idx_to_val_dict, val_to_idx_dict\n",
    "\n",
    "\n",
    "def val_to_idx(D_vals, max_vals, val_to_idx_dict):\n",
    "    \"\"\"\n",
    "    Converts training data from real values to index format using dictionaries.\n",
    "    Returns D_idxs (tensor with shape (k, num_choices, 1))\n",
    "        and max_idxs (tensor with shape (k, 1)):\n",
    "            max_idxs[i,0] is argmax of D_idxs[i,:,0]\n",
    "    :param D_vals: [k] list of ndarray [:,d]\n",
    "    :param max_vals: [k] list of ndarray [1,d]\n",
    "    \"\"\"\n",
    "\n",
    "    k = len(D_vals)\n",
    "\n",
    "    max_idxs = np.zeros(k, dtype=np.int32)\n",
    "\n",
    "    for i in range(k):\n",
    "        if max_vals[i] is not None:\n",
    "            diff = np.sum(np.square(D_vals[i] - max_vals[i]), axis=1)\n",
    "            max_idxs[i] = np.where(diff < 1e-30)[0]\n",
    "        else:\n",
    "            max_idxs[i] = -1\n",
    "\n",
    "    max_idxs = tf.constant(max_idxs)\n",
    "\n",
    "    D_idxs = tf.TensorArray(dtype=tf.int32, size=k, name='D_idxs', infer_shape=False, clear_after_read=False)\n",
    "\n",
    "    for i in range(k):\n",
    "        np.stack([ [val_to_idx_dict[tuple(datum)]] for datum in D_vals[i] ])\n",
    "\n",
    "    cond = lambda i, _: i < k\n",
    "    body = lambda i, D_idxs: \\\n",
    "        (i+1,\n",
    "         D_idxs.write(\n",
    "            i,\n",
    "            tf.constant([ val_to_idx_dict[tuple(datum)] for datum in D_vals[i] ], dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    _, D_idxs = tf.while_loop(cond, body, (0, D_idxs))\n",
    "\n",
    "    return D_idxs, max_idxs\n",
    "\n",
    "\n",
    "def train_model_fullcov(X,\n",
    "                        y,\n",
    "                        num_inducing,\n",
    "                        inducing_vars,\n",
    "                        obj_low,\n",
    "                        obj_high,\n",
    "                        lengthscale=1.,\n",
    "                        num_steps=5000,\n",
    "                        indifference_threshold=None):\n",
    "    \"\"\"\n",
    "    if indifference_threshold is None:\n",
    "        indifference_threshold is trained with maximum likelihood estimation\n",
    "    else:\n",
    "        indifference_threshold is fixed\n",
    "    :param X: np array with shape (num_data, num_choices, input_dims). Ordinal data\n",
    "    :param y: np array with shape (num_data, input_dims). Most preferred input for each set of inputs. Each y value must\n",
    "    match exactly to one of the choices in its corresponding X entry\n",
    "    :param num_inducing: number of inducing variables to use\n",
    "    :param obj_low: int. Floor of possible inducing point value in each dimension\n",
    "    :param obj_high: int. Floor of possible inducing point value in each dimension\n",
    "    :param lengthscale: float. Lengthscale to initialize RBF kernel with\n",
    "    :param num_steps: int that specifies how many optimization steps to take when training model\n",
    "    :param indifference_threshold:\n",
    "    \"\"\"\n",
    "    input_dims = X.shape[2]\n",
    "    idx_to_val_dict, val_to_idx_dict = populate_dicts(X)\n",
    "    D_idxs, max_idxs = val_to_idx(X, y, val_to_idx_dict)\n",
    "\n",
    "    n = len(val_to_idx_dict.keys())\n",
    "    inputs = np.array([idx_to_val_dict[i] for i in range(n)])\n",
    "\n",
    "    # Initialize variational parameters\n",
    "\n",
    "    q_mu = tf.Variable(np.zeros([num_inducing, 1]), name=\"q_mu\", dtype=tf.float64)\n",
    "    q_sqrt_latent = tf.Variable(np.expand_dims(np.eye(num_inducing), axis=0), name=\"q_sqrt_latent\", dtype=tf.float64)\n",
    "    kernel = gpflow.kernels.RBF(lengthscale=[lengthscale for i in range(input_dims)])\n",
    "    kernel.lengthscale.transform = gpflow.utilities.bijectors.positive(lower=gpflow.default_jitter())\n",
    "    u = tf.Variable(inducing_vars,\n",
    "                    name=\"u\",\n",
    "                    dtype=tf.float64)\n",
    "                    #constraint=lambda x: tf.clip_by_value(x, obj_low, obj_high))\n",
    "\n",
    "    is_threshold_trainable = (indifference_threshold is None)\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        indifference_threshold = tf.Variable(0.1, dtype=tf.float64,\n",
    "                        constraint=lambda x: tf.clip_by_value(x,\n",
    "                                                clip_value_min=0.0,\n",
    "                                                clip_value_max=np.infty))\n",
    "\n",
    "    neg_elbo = lambda: -elbo_fullcov(q_mu=q_mu,\n",
    "                                     q_sqrt_latent=q_sqrt_latent,\n",
    "                                     inducing_inputs=u,\n",
    "                                     D_idxs=D_idxs,\n",
    "                                     max_idxs=max_idxs,\n",
    "                                     kernel=kernel,\n",
    "                                     inputs=inputs,\n",
    "                                     indifference_threshold=indifference_threshold,\n",
    "                                     n_inducing_sample=50,\n",
    "                                     n_f_given_inducing_sample=50)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        print(\"Indifference_threshold is trainable.\")\n",
    "        trainable_vars = [q_mu, q_sqrt_latent, u, indifference_threshold] + list(kernel.trainable_variables)\n",
    "    else:\n",
    "        print(\"Indifference_threshold is fixed at {}\".format(indifference_threshold))\n",
    "        trainable_vars = [q_mu, q_sqrt_latent, u] + list(kernel.trainable_variables)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    vals = []\n",
    "\n",
    "    try:\n",
    "        for i in range(num_steps):\n",
    "            vals.append((q_mu.numpy(), q_sqrt_latent.numpy(), u.numpy(), inputs, kernel.lengthscale.numpy(), kernel.variance.numpy(), neg_elbo().numpy()))\n",
    "            print(i)\n",
    "            neg_elbo().numpy()\n",
    "            optimizer.minimize(neg_elbo, var_list=trainable_vars)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print('Negative ELBO at step {}: {} in {:.4f}s'.format(i,\n",
    "                           neg_elbo().numpy(),\n",
    "                           time.time() - start_time))\n",
    "\n",
    "                start_time = time.time()\n",
    "    except tf.errors.InvalidArgumentError as err:\n",
    "        print(err)\n",
    "        print(q_mu)\n",
    "        print(q_sqrt_latent)\n",
    "        print(u)\n",
    "        print(inputs)\n",
    "        gpflow.utilities.print_summary(kernel)\n",
    "        return vals\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        return q_mu, tf.linalg.band_part(q_sqrt_latent, -1, 0), u, inputs, kernel, indifference_threshold  # q_mu and q_sqrt\n",
    "    else:\n",
    "        return q_mu, tf.linalg.band_part(q_sqrt_latent, -1, 0), u, inputs, kernel  # q_mu and q_sqrt\n",
    "\n",
    "\n",
    "def init_SVGP_fullcov(q_mu, q_sqrt, inducing_variables, kernel, likelihood):\n",
    "    \"\"\"\n",
    "    Returns a gpflow SVGP model using the values obtained from train_model.\n",
    "    :param q_mu: np array or tensor of shape (num_inputs, 1)\n",
    "    :param q_sqrt: np array or tensor of shape (num_inputs, num_inputs). Lower triangular matrix\n",
    "    :param inducing_variables: tensor of shape (num_inducing, input_dims)\n",
    "    :param inputs: np array or tensor of shape (num_inputs, input_dims)\n",
    "    :param kernel: gpflow kernel\n",
    "    :param likelihood: gpflow likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    model = gpflow.models.SVGP(kernel=kernel,\n",
    "                               likelihood=likelihood,\n",
    "                               inducing_variable=inducing_variables,\n",
    "                               whiten=False)\n",
    "\n",
    "    model.q_mu.assign(q_mu)\n",
    "    model.q_sqrt.assign(q_sqrt)\n",
    "\n",
    "    # Set so that the parameters learned do not change if further optimization over\n",
    "    # other parameters is performed\n",
    "    set_trainable(model.q_mu, False)\n",
    "    set_trainable(model.q_sqrt, False)\n",
    "    set_trainable(model.inducing_variable.Z, False)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[0.54488318],\n",
    "        [0.4236548 ]],\n",
    "       [[0.54488318],\n",
    "        [0.64589411]],\n",
    "       [[0.4236548 ],\n",
    "        [0.64589411]],\n",
    "       [[0.000001        ],\n",
    "        [0.99884701]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[[0.54488318]],\n",
    "       [[0.64589411]],\n",
    "       [[0.64589411]],\n",
    "       [[0.000001        ]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_vars = np.array([[0.1],\n",
    "                          [0.2],\n",
    "                          [0.3],\n",
    "                          [0.]])  # This equal to one point in X, will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indifference_threshold is trainable.\n",
      "0\n",
      "WARNING:tensorflow:From /home/qphong/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/array_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "Negative ELBO at step 0: 3.5009392304921945 in 1.4855s\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "vals = train_model_fullcov(X, y, \n",
    "                             num_inducing=4,\n",
    "                             inducing_vars=inducing_vars,\n",
    "                             obj_low=0.,\n",
    "                             obj_high=1.,\n",
    "                             lengthscale=0.05,\n",
    "                             num_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed gradient for inducing variables u becomes nan, causing u to become nan as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[0.54488318],\n",
    "        [0.4236548 ]],\n",
    "       [[0.54488318],\n",
    "        [0.64589411]],\n",
    "       [[0.4236548 ],\n",
    "        [0.64589411]],\n",
    "       [[0.        ],\n",
    "        [0.99884701]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[[0.54488318]],\n",
    "       [[0.64589411]],\n",
    "       [[0.64589411]],\n",
    "       [[0.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_vars = np.array([[0.1],\n",
    "                          [0.2],\n",
    "                          [0.3],\n",
    "                          [0.99884701]])  # This equal to one point in X, will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed inducing point to another point in X, will also fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indifference_threshold is trainable.\n",
      "0\n",
      "Negative ELBO at step 0: 4.152891464047798 in 0.2083s\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-70f288c4b721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mobj_high\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                              \u001b[0mlengthscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                              num_steps=3000)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-9fd1f2da8f84>\u001b[0m in \u001b[0;36mtrain_model_fullcov\u001b[0;34m(X, y, num_inducing, inducing_vars, obj_low, obj_high, lengthscale, num_steps, indifference_threshold)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_sqrt_latent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlengthscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mneg_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_elbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9fd1f2da8f84>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    357\u001b[0m                                      \u001b[0mindifference_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindifference_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                                      \u001b[0mn_inducing_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                                      n_f_given_inducing_sample=50)\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9fd1f2da8f84>\u001b[0m in \u001b[0;36melbo_fullcov\u001b[0;34m(q_mu, q_sqrt_latent, inducing_inputs, D_idxs, max_idxs, kernel, inputs, indifference_threshold, n_inducing_sample, n_f_given_inducing_sample)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             parallel_iterations=10)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mklterm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2476\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9fd1f2da8f84>\u001b[0m in \u001b[0;36mbody\u001b[0;34m(i, likelihood)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# implementing a threshold for the choice of indifference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mmask_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_choice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mdiff_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmask_mat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mindifference_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/linalg_ops.py\u001b[0m in \u001b[0;36meye\u001b[0;34m(num_rows, num_columns, batch_shape, dtype, name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                              \u001b[0mbatch_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                              \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                              name=name)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/linalg_ops_impl.py\u001b[0m in \u001b[0;36meye\u001b[0;34m(num_rows, num_columns, batch_shape, dtype, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mdiag_ones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiag_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_square\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_diag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiag_ones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure it's a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(dims, value, name)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   \"\"\"\n\u001b[0;32m--> 233\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sebst/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(dims, value, name)\u001b[0m\n\u001b[1;32m   3233\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   3234\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fill\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3235\u001b[0;31m         dims, value)\n\u001b[0m\u001b[1;32m   3236\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vals1 = train_model_fullcov(X, y, \n",
    "                             num_inducing=4,\n",
    "                             inducing_vars=inducing_vars,\n",
    "                             obj_low=0.,\n",
    "                             obj_high=1.,\n",
    "                             lengthscale=0.05,\n",
    "                             num_steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
