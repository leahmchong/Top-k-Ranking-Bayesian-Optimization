{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from gpflow.utilities import set_trainable, print_summary\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "sys.path.append(os.path.split(os.path.split(os.getcwd())[0])[0]) # Move 3 levels up directory to import PBO\n",
    "import PBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_stochastic.py. Changed train_model_fullcov to take pre-determined inducing inputs for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "#     1. use samples to compute the objective function\n",
    "#         elbo_fullcov(q_mu, q_sqrt_latent, inducing_variables, D_idxs, max_idxs, kernel, inputs)\n",
    "#        then it can works for different number of choices\n",
    "#     2. val_to_idxs, populate_dicts: need to adaptive to different number of choices\n",
    "#         by converting some to list\n",
    "#     2. check function q_f to see if the implementation is correct\n",
    "#         checked: correct\n",
    "\"\"\"\n",
    "    forester_get_Y:\n",
    "        as X is a list, change the function!\n",
    "    sample maximizers:\n",
    "        change observations from inducing input, inducing variables\n",
    "        to distribution of inducing variables\n",
    "\n",
    "Given ordinal (preference) data consisting of sets of input points and a most preferred input point for every such set,\n",
    "the train_model function learns variational parameters that approximate the distribution of a latent function f over\n",
    "all input points present in the data, which can be used to construct GP models to approximate f over the entire input\n",
    "space.\n",
    "Formulation by Nguyen Quoc Phong.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import gpflow\n",
    "from gpflow.utilities import set_trainable\n",
    "\n",
    "\n",
    "def elbo_fullcov(q_mu,\n",
    "                q_sqrt_latent,\n",
    "                inducing_inputs,\n",
    "                D_idxs,\n",
    "                max_idxs,\n",
    "                kernel,\n",
    "                inputs,\n",
    "                indifference_threshold,\n",
    "                n_inducing_sample=50,\n",
    "                n_f_given_inducing_sample=30):\n",
    "    \"\"\"\n",
    "    Calculates the ELBO for the PBO formulation, using a full covariance matrix.\n",
    "    :param q_mu: tensor with shape (num_inducing, 1)\n",
    "    :param q_sqrt_latent: tensor with shape (1, num_inducing, num_inducing). Will be forced into lower triangular\n",
    "        matrix such that q_sqrt @ q_sqrt^T represents the covariance matrix of inducing variables\n",
    "    :param inducing_inputs: tensor with shape (num_inducing, input_dims)\n",
    "    :param D_idxs: tensor with shape (num_data, num_choices, 1)\n",
    "        Input data points, that are indices into q_mu and q_var for tf.gather_nd\n",
    "    :param max_idxs: tensor with shape (num_data, 1)\n",
    "        Selection of most preferred input point for each collection of data points, that are indices into\n",
    "        q_mu and q_var\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: tensor of shape ()\n",
    "    \"\"\"\n",
    "    Kmm = kernel.K(inducing_inputs)\n",
    "\n",
    "    logdet_Kmm = tf.linalg.logdet(Kmm)\n",
    "    invKmm = cholesky_matrix_inverse(Kmm)\n",
    "\n",
    "    num_data = D_idxs.size()\n",
    "\n",
    "    # 1. Sample from q(u)\n",
    "    standard_mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=tf.zeros(tf.shape(q_mu)[0], dtype=tf.float64),\n",
    "            scale_diag=tf.ones(tf.shape(q_mu)[0], dtype=tf.float64))\n",
    "\n",
    "    standard_mvn_samples = standard_mvn.sample(n_inducing_sample)\n",
    "    # (n_inducing_sample, num_inducing)\n",
    "\n",
    "    q_sqrt = tf.linalg.band_part(q_sqrt_latent, -1, 0)  # Force into lower triangular\n",
    "    q_full = q_sqrt @ tf.linalg.matrix_transpose(q_sqrt)  # (1, num_data, num_data)\n",
    "    inv_q_full = cholesky_matrix_inverse(tf.squeeze(q_full, axis=0))\n",
    "\n",
    "    posterior_inducing_samples = q_sqrt @ tf.expand_dims(standard_mvn_samples, axis=-1) + q_mu\n",
    "    # (n_inducing_sample, num_inducing, 1)\n",
    "\n",
    "    # 2. p(f|u) where u are samples from q(u)\n",
    "    f_mean_given_inducing_sample, f_cov_given_inducing_sample = p_f_given_u(\n",
    "            posterior_inducing_samples,\n",
    "            inducing_inputs, kernel, inputs, invKmm)\n",
    "    # f_mean: (n_inducing_sample, num_inputs)\n",
    "    # f_cov: (num_inputs, num_inputs)\n",
    "\n",
    "    # 3. KL[q(u) || p(u)] = E_{q(u)} [log q(u)] - [log p(u)]\n",
    "    zero_mean_inducing_samples = posterior_inducing_samples - q_mu\n",
    "    # (n_inducing_sample, num_inducing, 1)\n",
    "\n",
    "    klterm = -0.5 * tf.reduce_mean(\n",
    "        tf.linalg.logdet(q_full)\n",
    "        + tf.transpose(zero_mean_inducing_samples, perm=[0,2,1]) @ inv_q_full @ zero_mean_inducing_samples\n",
    "        - logdet_Kmm\n",
    "        - tf.transpose(posterior_inducing_samples, perm= [0,2,1]) @ invKmm @ posterior_inducing_samples\n",
    "    )\n",
    "\n",
    "    def body(i, likelihood):\n",
    "        idxs = tf.squeeze(D_idxs.read(i))\n",
    "        max_idx = max_idxs[i]\n",
    "        num_choice = tf.shape(idxs)[0]\n",
    "\n",
    "        fi_cov = tf.gather(\n",
    "            tf.gather(f_cov_given_inducing_sample, indices=idxs, axis=0),\n",
    "            indices=idxs, axis=1)\n",
    "        # (num_choice, num_choice)\n",
    "\n",
    "        fi_mean = tf.gather(f_mean_given_inducing_sample, indices=idxs, axis=1)\n",
    "        # (n_inducing_sample, num_choice)\n",
    "\n",
    "        standard_mvn_i = tfp.distributions.MultivariateNormalDiag(\n",
    "                loc=tf.zeros_like(idxs, dtype=tf.float64),\n",
    "                scale_diag=tf.ones_like(idxs, dtype=tf.float64))\n",
    "\n",
    "        standard_mvn_i_samples = standard_mvn_i.sample(n_f_given_inducing_sample)\n",
    "        # (n_f_given_inducing_sample, num_choice)\n",
    "\n",
    "        eigvalues, eigvects = tf.linalg.eigh(fi_cov)\n",
    "        eigvalues = tf.clip_by_value(eigvalues, clip_value_min=0., clip_value_max=np.infty)\n",
    "        transform_mat = eigvects @ tf.linalg.diag(tf.sqrt(eigvalues))\n",
    "\n",
    "        zero_mean_f_samples = tf.squeeze(transform_mat @ tf.expand_dims(standard_mvn_i_samples, axis=-1), axis=-1)\n",
    "        # (n_f_given_inducing_sample, num_choice)\n",
    "\n",
    "        f_samples = tf.expand_dims(zero_mean_f_samples, axis=1) + fi_mean\n",
    "        # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "        # implementing a threshold for the choice of indifference\n",
    "        mask_mat = tf.eye(num_choice, dtype=tf.float64)\n",
    "        diff_mat = (1.0 - mask_mat) * indifference_threshold\n",
    "\n",
    "        def true_fn(max_idx, f_samples, diff_mat):\n",
    "            f_samples = f_samples + tf.gather(diff_mat, indices=max_idx, axis=0)\n",
    "\n",
    "            return tf.reduce_mean(\n",
    "                    tf.gather(f_samples, indices=max_idx, axis=-1)\n",
    "                    - tf.reduce_logsumexp(f_samples, axis=-1))\n",
    "\n",
    "        def false_fn(f_samples, mask_mat, diff_mat):\n",
    "            max_idx_f_samples = tf.squeeze(mask_mat @ tf.expand_dims(f_samples, axis=-1), axis=-1)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "            all_f_samples = tf.expand_dims(f_samples, axis=-1) + tf.gather(diff_mat, indices=max_idx, axis=0)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice, num_choice)\n",
    "\n",
    "            all_choice_logprob = max_idx_f_samples - tf.reduce_logsumexp(all_f_samples, axis=-2)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample, num_choice)\n",
    "\n",
    "            indifference_prob = 1.0 - tf.exp( tf.reduce_logsumexp(all_choice_logprob, axis=-1) )\n",
    "            indifference_prob = tf.clip_by_value(indifference_prob, clip_value_min=1e-50, clip_value_max=1.0 - 1e-50)\n",
    "            indifference_logprob = tf.math.log(indifference_prob)\n",
    "            # (n_f_given_inducing_sample, n_inducing_sample)\n",
    "\n",
    "            return tf.reduce_mean( indifference_logprob )\n",
    "\n",
    "        likelihood_i = tf.cond(max_idx >= 0,\n",
    "            lambda: true_fn(max_idx, f_samples, diff_mat),\n",
    "            lambda: false_fn(f_samples, mask_mat, diff_mat))\n",
    "\n",
    "        likelihood = likelihood + likelihood_i\n",
    "\n",
    "        return i+1, likelihood\n",
    "\n",
    "    cond = lambda i, _: i < num_data\n",
    "\n",
    "    _, likelihood = tf.while_loop(\n",
    "            cond,\n",
    "            body,\n",
    "            (0, tf.constant(0.0, dtype=tf.float64)),\n",
    "            parallel_iterations=10)\n",
    "\n",
    "    elbo = likelihood - klterm\n",
    "\n",
    "    return elbo\n",
    "\n",
    "\n",
    "def cholesky_matrix_inverse(A):\n",
    "    \"\"\"\n",
    "    :param A: Symmetric positive-definite matrix, tensor of shape (n, n)\n",
    "    :return: Inverse of A, tensor of shape (n, n)\n",
    "    \"\"\"\n",
    "    L = tf.linalg.cholesky(A)\n",
    "    L_inv = tf.linalg.triangular_solve(L, tf.eye(A.shape[0], dtype=tf.float64))\n",
    "    return tf.linalg.matrix_transpose(L_inv) @ L_inv\n",
    "\n",
    "\n",
    "def p_f_given_u(inducing_vars, inducing_inputs, kernel, inputs, invKmm_prior):\n",
    "    \"\"\"\n",
    "    Calculates the mean and covariance of the joint distriubtion p(f|u)\n",
    "    :param inducing_vars: tensor with shape (nsample,num_inducing,1)\n",
    "    :param inducing_inputs: tensor with shape (num_inducing, input_dims)\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: (tensor of shape (nsample,num_inputs), tensor of shape (num_inputs, num_inputs))\n",
    "    \"\"\"\n",
    "    Knm = kernel.K(inputs, inducing_inputs)  # (n, m)\n",
    "    A = Knm @ invKmm_prior  # (n, m)\n",
    "\n",
    "    f_mean = tf.squeeze(A @ inducing_vars, axis=-1)\n",
    "    # (nsample, num_inducing)\n",
    "\n",
    "    Knn = kernel.K(inputs)\n",
    "    f_cov = Knn - A @ tf.transpose(Knm)\n",
    "    # (num_inputs, num_inputs)\n",
    "\n",
    "    return f_mean, f_cov\n",
    "\n",
    "\n",
    "def q_f(q_mu, q_sqrt_latent, inducing_variables, kernel, inputs):\n",
    "    \"\"\"\n",
    "    Calculates the mean and covariance of the joint distribution q(f)\n",
    "    :param q_mu: tensor with shape (num_inducing, 1)\n",
    "    :param q_sqrt_latent: tensor with shape (1, num_inducing, num_inducing). Will be forced into lower triangular\n",
    "        matrix such that q_sqrt @ q_sqrt^T represents the covariance matrix of inducing variables\n",
    "    :param inducing_variables: tensor with shape (num_inducing, input_dims)\n",
    "    :param kernel: gpflow kernel to calculate covariance matrix for KL divergence\n",
    "    :param inputs: tensor of shape (num_inputs, input_dims) with indices corresponding to that of D_idxs and max_idxs\n",
    "    :return: (tensor of shape (num_inputs), tensor of shape (num_inputs, num_inputs))\n",
    "    \"\"\"\n",
    "\n",
    "    q_sqrt = tf.linalg.band_part(q_sqrt_latent, -1, 0)  # Force into lower triangular\n",
    "    q_full = q_sqrt @ tf.linalg.matrix_transpose(q_sqrt)  # (1, num_data, num_data)\n",
    "\n",
    "    Kmm = kernel.K(inducing_variables)  # (m, m)\n",
    "    Kmm_inv = cholesky_matrix_inverse(Kmm)\n",
    "\n",
    "    Knm = kernel.K(inputs, inducing_variables)  # (n, m)\n",
    "    A = Knm @ Kmm_inv  # (n, m)\n",
    "\n",
    "    f_mean = tf.squeeze(A @ q_mu, axis=-1)\n",
    "\n",
    "    Knn = kernel.K(inputs)\n",
    "    S = tf.squeeze(q_full, axis=0)\n",
    "    f_cov = Knn + (A @ (S - Kmm) @ tf.linalg.matrix_transpose(A))\n",
    "\n",
    "    return f_mean, f_cov\n",
    "\n",
    "\n",
    "def populate_dicts(D_vals):\n",
    "    \"\"\"\n",
    "    Populates dictionaries to assign an index to each value seen in the training data.\n",
    "    :param D_vals: [k] list of 2-d ndarray [:,d] (to allow different num_choice for different observations)\n",
    "    \"\"\"\n",
    "    idx_to_val_dict = {}\n",
    "    val_to_idx_dict = {}\n",
    "\n",
    "    D_vals_list = np.concatenate(D_vals, axis=0)\n",
    "    D_vals_list_tuples = [tuple(i) for i in D_vals_list]\n",
    "    D_vals_set = set(D_vals_list_tuples)\n",
    "\n",
    "    for val in D_vals_set:\n",
    "        val_to_idx_dict[val] = len(val_to_idx_dict)\n",
    "        idx_to_val_dict[len(val_to_idx_dict)-1] = val\n",
    "\n",
    "    return idx_to_val_dict, val_to_idx_dict\n",
    "\n",
    "\n",
    "def val_to_idx(D_vals, max_vals, val_to_idx_dict):\n",
    "    \"\"\"\n",
    "    Converts training data from real values to index format using dictionaries.\n",
    "    Returns D_idxs (tensor with shape (k, num_choices, 1))\n",
    "        and max_idxs (tensor with shape (k, 1)):\n",
    "            max_idxs[i,0] is argmax of D_idxs[i,:,0]\n",
    "    :param D_vals: [k] list of ndarray [:,d]\n",
    "    :param max_vals: [k] list of ndarray [1,d]\n",
    "    \"\"\"\n",
    "\n",
    "    k = len(D_vals)\n",
    "\n",
    "    max_idxs = np.zeros(k, dtype=np.int32)\n",
    "\n",
    "    for i in range(k):\n",
    "        if max_vals[i] is not None:\n",
    "            diff = np.sum(np.square(D_vals[i] - max_vals[i]), axis=1)\n",
    "            max_idxs[i] = np.where(diff < 1e-30)[0]\n",
    "        else:\n",
    "            max_idxs[i] = -1\n",
    "\n",
    "    max_idxs = tf.constant(max_idxs)\n",
    "\n",
    "    D_idxs = tf.TensorArray(dtype=tf.int32, size=k, name='D_idxs', infer_shape=False, clear_after_read=False)\n",
    "\n",
    "    for i in range(k):\n",
    "        np.stack([ [val_to_idx_dict[tuple(datum)]] for datum in D_vals[i] ])\n",
    "\n",
    "    cond = lambda i, _: i < k\n",
    "    body = lambda i, D_idxs: \\\n",
    "        (i+1,\n",
    "         D_idxs.write(\n",
    "            i,\n",
    "            tf.constant([ val_to_idx_dict[tuple(datum)] for datum in D_vals[i] ], dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    _, D_idxs = tf.while_loop(cond, body, (0, D_idxs))\n",
    "\n",
    "    return D_idxs, max_idxs\n",
    "\n",
    "\n",
    "def train_model_fullcov(X,\n",
    "                        y,\n",
    "                        num_inducing,\n",
    "                        inducing_vars,\n",
    "                        obj_low,\n",
    "                        obj_high,\n",
    "                        lengthscale=1.,\n",
    "                        num_steps=5000,\n",
    "                        indifference_threshold=None):\n",
    "    \"\"\"\n",
    "    if indifference_threshold is None:\n",
    "        indifference_threshold is trained with maximum likelihood estimation\n",
    "    else:\n",
    "        indifference_threshold is fixed\n",
    "    :param X: np array with shape (num_data, num_choices, input_dims). Ordinal data\n",
    "    :param y: np array with shape (num_data, input_dims). Most preferred input for each set of inputs. Each y value must\n",
    "    match exactly to one of the choices in its corresponding X entry\n",
    "    :param num_inducing: number of inducing variables to use\n",
    "    :param obj_low: int. Floor of possible inducing point value in each dimension\n",
    "    :param obj_high: int. Floor of possible inducing point value in each dimension\n",
    "    :param lengthscale: float. Lengthscale to initialize RBF kernel with\n",
    "    :param num_steps: int that specifies how many optimization steps to take when training model\n",
    "    :param indifference_threshold:\n",
    "    \"\"\"\n",
    "    input_dims = X.shape[2]\n",
    "    idx_to_val_dict, val_to_idx_dict = populate_dicts(X)\n",
    "    D_idxs, max_idxs = val_to_idx(X, y, val_to_idx_dict)\n",
    "\n",
    "    n = len(val_to_idx_dict.keys())\n",
    "    inputs = np.array([idx_to_val_dict[i] for i in range(n)])\n",
    "\n",
    "    # Initialize variational parameters\n",
    "\n",
    "    q_mu = tf.Variable(np.zeros([num_inducing, 1]), name=\"q_mu\", dtype=tf.float64)\n",
    "    q_sqrt_latent = tf.Variable(np.expand_dims(np.eye(num_inducing), axis=0), name=\"q_sqrt_latent\", dtype=tf.float64)\n",
    "    kernel = gpflow.kernels.RBF(lengthscale=[lengthscale for i in range(input_dims)])\n",
    "    kernel.lengthscale.transform = gpflow.utilities.bijectors.positive(lower=gpflow.default_jitter())\n",
    "    u = tf.Variable(inducing_vars,\n",
    "                    name=\"u\",\n",
    "                    dtype=tf.float64)\n",
    "                    #constraint=lambda x: tf.clip_by_value(x, obj_low, obj_high))\n",
    "\n",
    "    is_threshold_trainable = (indifference_threshold is None)\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        indifference_threshold = tf.Variable(0.1, dtype=tf.float64,\n",
    "                        constraint=lambda x: tf.clip_by_value(x,\n",
    "                                                clip_value_min=0.0,\n",
    "                                                clip_value_max=np.infty))\n",
    "\n",
    "    neg_elbo = lambda: -elbo_fullcov(q_mu=q_mu,\n",
    "                                     q_sqrt_latent=q_sqrt_latent,\n",
    "                                     inducing_inputs=u,\n",
    "                                     D_idxs=D_idxs,\n",
    "                                     max_idxs=max_idxs,\n",
    "                                     kernel=kernel,\n",
    "                                     inputs=inputs,\n",
    "                                     indifference_threshold=indifference_threshold,\n",
    "                                     n_inducing_sample=50,\n",
    "                                     n_f_given_inducing_sample=50)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        print(\"Indifference_threshold is trainable.\")\n",
    "        trainable_vars = [q_mu, q_sqrt_latent, u, indifference_threshold] + list(kernel.trainable_variables)\n",
    "    else:\n",
    "        print(\"Indifference_threshold is fixed at {}\".format(indifference_threshold))\n",
    "        trainable_vars = [q_mu, q_sqrt_latent, u] + list(kernel.trainable_variables)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    vals = []\n",
    "\n",
    "    try:\n",
    "        for i in range(num_steps):\n",
    "            vals.append((q_mu.numpy(), q_sqrt_latent.numpy(), u.numpy(), inputs, kernel.lengthscale.numpy(), kernel.variance.numpy(), neg_elbo().numpy()))\n",
    "            print(i)\n",
    "            neg_elbo().numpy()\n",
    "            optimizer.minimize(neg_elbo, var_list=trainable_vars)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print('Negative ELBO at step {}: {} in {:.4f}s'.format(i,\n",
    "                           neg_elbo().numpy(),\n",
    "                           time.time() - start_time))\n",
    "\n",
    "                start_time = time.time()\n",
    "    except tf.errors.InvalidArgumentError as err:\n",
    "        print(err)\n",
    "        print(q_mu)\n",
    "        print(q_sqrt_latent)\n",
    "        print(u)\n",
    "        print(inputs)\n",
    "        gpflow.utilities.print_summary(kernel)\n",
    "        return vals\n",
    "\n",
    "    if is_threshold_trainable:\n",
    "        return q_mu, tf.linalg.band_part(q_sqrt_latent, -1, 0), u, inputs, kernel, indifference_threshold  # q_mu and q_sqrt\n",
    "    else:\n",
    "        return q_mu, tf.linalg.band_part(q_sqrt_latent, -1, 0), u, inputs, kernel  # q_mu and q_sqrt\n",
    "\n",
    "\n",
    "def init_SVGP_fullcov(q_mu, q_sqrt, inducing_variables, kernel, likelihood):\n",
    "    \"\"\"\n",
    "    Returns a gpflow SVGP model using the values obtained from train_model.\n",
    "    :param q_mu: np array or tensor of shape (num_inputs, 1)\n",
    "    :param q_sqrt: np array or tensor of shape (num_inputs, num_inputs). Lower triangular matrix\n",
    "    :param inducing_variables: tensor of shape (num_inducing, input_dims)\n",
    "    :param inputs: np array or tensor of shape (num_inputs, input_dims)\n",
    "    :param kernel: gpflow kernel\n",
    "    :param likelihood: gpflow likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    model = gpflow.models.SVGP(kernel=kernel,\n",
    "                               likelihood=likelihood,\n",
    "                               inducing_variable=inducing_variables,\n",
    "                               whiten=False)\n",
    "\n",
    "    model.q_mu.assign(q_mu)\n",
    "    model.q_sqrt.assign(q_sqrt)\n",
    "\n",
    "    # Set so that the parameters learned do not change if further optimization over\n",
    "    # other parameters is performed\n",
    "    set_trainable(model.q_mu, False)\n",
    "    set_trainable(model.q_sqrt, False)\n",
    "    set_trainable(model.inducing_variable.Z, False)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[0.54488318],\n",
    "        [0.4236548 ]],\n",
    "       [[0.54488318],\n",
    "        [0.64589411]],\n",
    "       [[0.4236548 ],\n",
    "        [0.64589411]],\n",
    "       [[0.000001        ],\n",
    "        [0.99884701]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[[0.54488318]],\n",
    "       [[0.64589411]],\n",
    "       [[0.64589411]],\n",
    "       [[0.000001        ]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_vars = np.array([[0.1],\n",
    "                          [0.2],\n",
    "                          [0.3],\n",
    "                          [0.]])  # This equal to one point in X, will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indifference_threshold is trainable.\n",
      "0\n",
      "Negative ELBO at step 0: 4.192925159405133 in 0.2791s\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "vals = train_model_fullcov(X, y, \n",
    "                             num_inducing=4,\n",
    "                             inducing_vars=inducing_vars,\n",
    "                             obj_low=0.,\n",
    "                             obj_high=1.,\n",
    "                             lengthscale=0.05,\n",
    "                             num_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed gradient for inducing variables u becomes nan, causing u to become nan as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[[0.54488318],\n",
    "        [0.4236548 ]],\n",
    "       [[0.54488318],\n",
    "        [0.64589411]],\n",
    "       [[0.4236548 ],\n",
    "        [0.64589411]],\n",
    "       [[0.        ],\n",
    "        [0.99884701]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[[0.54488318]],\n",
    "       [[0.64589411]],\n",
    "       [[0.64589411]],\n",
    "       [[0.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_vars = np.array([[0.1],\n",
    "                          [0.2],\n",
    "                          [0.3],\n",
    "                          [0.99884701]])  # This equal to one point in X, will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed inducing point to another point in X, will also fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indifference_threshold is trainable.\n",
      "0\n",
      "Input matrix is not invertible. [Op:MatrixTriangularSolve]\n",
      "<tf.Variable 'q_mu:0' shape=(4, 1) dtype=float64, numpy=\n",
      "array([[-0.00099999],\n",
      "       [-0.00099992],\n",
      "       [ 0.00099997],\n",
      "       [-0.00099999]])>\n",
      "<tf.Variable 'q_sqrt_latent:0' shape=(1, 4, 4) dtype=float64, numpy=\n",
      "array([[[ 0.99900002,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.00099993,  0.99900001,  0.        ,  0.        ],\n",
      "        [-0.00099967,  0.00099999,  0.99900023,  0.        ],\n",
      "        [-0.00099999, -0.00099998,  0.0009999 ,  0.99900001]]])>\n",
      "<tf.Variable 'u:0' shape=(4, 1) dtype=float64, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [nan],\n",
      "       [nan]])>\n",
      "[[0.64589411]\n",
      " [0.54488318]\n",
      " [0.4236548 ]\n",
      " [0.99884701]\n",
      " [0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                          </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SquaredExponential.variance   </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>nan    </td></tr>\n",
       "<tr><td>SquaredExponential.lengthscale</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[nan]  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals1 = train_model_fullcov(X, y, \n",
    "                             num_inducing=4,\n",
    "                             inducing_vars=inducing_vars,\n",
    "                             obj_low=0.,\n",
    "                             obj_high=1.,\n",
    "                             lengthscale=0.05,\n",
    "                             num_steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
